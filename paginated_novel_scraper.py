#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†é ç« ç¯€çˆ¬èŸ² - å°ˆé–€è™•ç†Novel543ç­‰æœ‰åˆ†é çš„å°èªªç¶²ç«™
è‡ªå‹•æª¢æ¸¬ä¸¦çˆ¬å–æ‰€æœ‰åˆ†é å…§å®¹ï¼Œæ”¯æŒå¤šç¨®é©—è­‰æ©Ÿåˆ¶

æ”¯æŒçš„é©—è­‰é¡å‹ï¼š
- reCAPTCHA (éœ€è¦æ‰‹å‹•å®Œæˆ)
- æ»‘å‹•é©—è­‰ (è‡ªå‹•è™•ç†)
- æŒ‰éˆ•é»æ“Šé©—è­‰ (è‡ªå‹•è™•ç†)
- CloudFlareé©—è­‰ (è‡ªå‹•ç­‰å¾…)

ä½¿ç”¨ç¯„ä¾‹ï¼š
    # åŸºæœ¬ä½¿ç”¨
    python scraper.py input.csv --start 1 --end 10
    
    # è™•ç†æœ‰é©—è­‰çš„ç¶²ç«™
    python scraper.py input.csv --start 1 --end 10 --verify-timeout 60
    
    # é—œé–‰è‡ªå‹•é©—è­‰ï¼ˆé‡åˆ°é©—è­‰å•é¡Œæ™‚ï¼‰
    python scraper.py input.csv --start 1 --end 10 --no-verify
    
    # ç„¡é ­æ¨¡å¼ï¼ˆä¸æ¨è–¦ç”¨æ–¼æœ‰é©—è­‰çš„ç¶²ç«™ï¼‰
    python scraper.py input.csv --start 1 --end 10 --headless --no-verify
"""

import argparse
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, WebDriverException
import time
import os
import json
import logging
import random
import sys
import re
from pathlib import Path
from urllib.parse import urlparse, urlunparse

class PaginatedNovelScraper:
    def __init__(self, csv_file_path, output_dir="paginated_novels", headless=False, auto_verify=True):
        """
        åˆå§‹åŒ–åˆ†é å°èªªçˆ¬èŸ²
        """
        self.csv_file_path = csv_file_path
        self.output_dir = output_dir
        self.headless = headless
        self.auto_verify = auto_verify
        self.verification_timeout = 30  # é è¨­é©—è­‰è¶…æ™‚æ™‚é–“
        self.driver = None
        
        # åˆ†é æª¢æ¸¬çš„æ­£å‰‡è¡¨é”å¼
        self.pagination_patterns = [
            r'\((\d+)/(\d+)\)',  # (1/2), (2/3) ç­‰
            r'ç¬¬(\d+)é /å…±(\d+)é ',  # ç¬¬1é /å…±3é 
            r'(\d+)/(\d+)é ',    # 1/3é 
        ]
        
        # å¸¸è¦‹é©—è­‰å…ƒç´ çš„é¸æ“‡å™¨
        self.verification_selectors = [
            # reCAPTCHA
            'iframe[src*="recaptcha"]',
            '.recaptcha-checkbox',
            '#recaptcha-anchor',
            
            # å¸¸è¦‹çš„"æˆ‘ä¸æ˜¯æ©Ÿå™¨äºº"æŒ‰éˆ•
            'button:contains("æˆ‘ä¸æ˜¯æ©Ÿå™¨äºº")',
            'button:contains("I\'m not a robot")',
            'input[value*="æˆ‘ä¸æ˜¯æ©Ÿå™¨äºº"]',
            'input[value*="not a robot"]',
            
            # é€šç”¨é©—è­‰æŒ‰éˆ•
            'button[class*="verify"]',
            'button[id*="verify"]',
            '.verify-button',
            '#verify-btn',
            
            # é»æ“Šç¢ºèªæŒ‰éˆ•
            'button:contains("ç¢ºèª")',
            'button:contains("ç¢ºå®š")',
            'button:contains("ç»§ç»­")',
            'button:contains("Continue")',
            'button:contains("Submit")',
            
            # æ»‘å‹•é©—è­‰
            '.slider-verify',
            '.slide-verify',
            '[class*="slider"]',
            
            # CloudFlareé©—è­‰
            '#challenge-stage',
            '.challenge-form',
            
            # è‡ªå®šç¾©é©—è­‰
            'button[onclick*="verify"]',
            'div[class*="human-verification"]',
            '.human-check',
        ]
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        self.setup_logging()

    def setup_logging(self):
        """è¨­ç½®æ—¥èªŒç³»çµ±"""
        log_file = Path(self.output_dir) / 'paginated_scraping.log'
        
        logging.getLogger().handlers.clear()
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)

    def setup_driver(self):
        """è¨­ç½®Chromeç€è¦½å™¨é©…å‹•"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            chrome_options.add_argument(f'--user-agent={user_agent}')
            
            if self.headless:
                chrome_options.add_argument('--headless')
                self.logger.info("é‹è¡Œåœ¨ç„¡é ­æ¨¡å¼")
            
            try:
                from webdriver_manager.chrome import ChromeDriverManager
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
            except ImportError:
                self.driver = webdriver.Chrome(options=chrome_options)
            
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.logger.info("ç€è¦½å™¨é©…å‹•è¨­ç½®æˆåŠŸ")
            return True
            
        except Exception as e:
            self.logger.error(f"ç€è¦½å™¨é©…å‹•è¨­ç½®å¤±æ•—: {e}")
            return False

    def wait_for_page_load(self, timeout=10):
        """ç­‰å¾…é é¢å®Œå…¨åŠ è¼‰"""
        try:
            WebDriverWait(self.driver, timeout).until(
                lambda driver: driver.execute_script("return document.readyState") == "complete"
            )
            return True
        except TimeoutException:
            self.logger.warning("é é¢åŠ è¼‰è¶…æ™‚")
            return False

    def detect_verification_elements(self):
        """æª¢æ¸¬é é¢ä¸­çš„é©—è­‰å…ƒç´ """
        verification_elements = []
        
        for selector in self.verification_selectors:
            try:
                # è™•ç†åŒ…å«æ–‡æœ¬çš„é¸æ“‡å™¨
                if ':contains(' in selector:
                    # æå–æ–‡æœ¬å…§å®¹
                    text = selector.split(':contains("')[1].split('")')[0]
                    # ä½¿ç”¨XPathæŸ¥æ‰¾åŒ…å«ç‰¹å®šæ–‡æœ¬çš„å…ƒç´ 
                    xpath = f"//*[contains(text(), '{text}')]"
                    elements = self.driver.find_elements(By.XPATH, xpath)
                else:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                
                for element in elements:
                    if element.is_displayed() and element.is_enabled():
                        verification_elements.append({
                            'element': element,
                            'selector': selector,
                            'type': self.classify_verification_type(selector)
                        })
                        
            except Exception as e:
                self.logger.debug(f"æª¢æ¸¬é©—è­‰å…ƒç´ å¤±æ•— {selector}: {e}")
                continue
        
        return verification_elements

    def classify_verification_type(self, selector):
        """åˆ†é¡é©—è­‰é¡å‹"""
        selector_lower = selector.lower()
        
        if 'recaptcha' in selector_lower:
            return 'recaptcha'
        elif 'slider' in selector_lower or 'slide' in selector_lower:
            return 'slider'
        elif 'challenge' in selector_lower:
            return 'cloudflare'
        elif any(word in selector_lower for word in ['verify', 'human', 'robot']):
            return 'human_verification'
        else:
            return 'button_click'

    def handle_verification(self, max_attempts=3, manual_timeout=None):
        """
        è™•ç†é é¢é©—è­‰
        
        Args:
            max_attempts: æœ€å¤§å˜—è©¦æ¬¡æ•¸
            manual_timeout: æ‰‹å‹•é©—è­‰è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰ï¼Œå¦‚æœç‚ºNoneå‰‡ä½¿ç”¨self.verification_timeout
        """
        if not self.auto_verify:
            return True
        
        if manual_timeout is None:
            manual_timeout = self.verification_timeout
            
        for attempt in range(max_attempts):
            try:
                # ç­‰å¾…é é¢ç©©å®š
                time.sleep(2)
                
                # æª¢æ¸¬é©—è­‰å…ƒç´ 
                verification_elements = self.detect_verification_elements()
                
                if not verification_elements:
                    self.logger.debug("æœªæª¢æ¸¬åˆ°é©—è­‰å…ƒç´ ")
                    return True
                
                self.logger.info(f"ğŸ” æª¢æ¸¬åˆ° {len(verification_elements)} å€‹é©—è­‰å…ƒç´ ")
                
                for i, ver_element in enumerate(verification_elements):
                    element = ver_element['element']
                    ver_type = ver_element['type']
                    selector = ver_element['selector']
                    
                    self.logger.info(f"  è™•ç†é©—è­‰ {i+1}: {ver_type}")
                    
                    if ver_type == 'recaptcha':
                        success = self.handle_recaptcha(element, manual_timeout)
                    elif ver_type == 'slider':
                        success = self.handle_slider_verification(element)
                    elif ver_type == 'cloudflare':
                        success = self.handle_cloudflare(manual_timeout)
                    else:
                        success = self.handle_button_click(element)
                    
                    if success:
                        self.logger.info(f"  âœ… é©—è­‰æˆåŠŸ: {ver_type}")
                        # ç­‰å¾…é©—è­‰å¾Œçš„é é¢è®ŠåŒ–
                        time.sleep(3)
                        return True
                    else:
                        self.logger.warning(f"  âŒ é©—è­‰å¤±æ•—: {ver_type}")
                
                # å¦‚æœæ‰€æœ‰é©—è­‰éƒ½å¤±æ•—ï¼Œå˜—è©¦ä¸‹ä¸€è¼ª
                if attempt < max_attempts - 1:
                    self.logger.info(f"å˜—è©¦ {attempt + 1}/{max_attempts} å¤±æ•—ï¼Œé‡è©¦...")
                    time.sleep(5)
                
            except Exception as e:
                self.logger.error(f"è™•ç†é©—è­‰æ™‚å‡ºéŒ¯: {e}")
                if attempt < max_attempts - 1:
                    time.sleep(5)
        
        self.logger.warning("æ‰€æœ‰é©—è­‰å˜—è©¦éƒ½å¤±æ•—äº†")
        return False

    def handle_button_click(self, element):
        """è™•ç†ç°¡å–®çš„æŒ‰éˆ•é»æ“Šé©—è­‰"""
        try:
            # æ»¾å‹•åˆ°å…ƒç´ ä½ç½®
            self.driver.execute_script("arguments[0].scrollIntoView(true);", element)
            time.sleep(1)
            
            # é»æ“Šå…ƒç´ 
            element.click()
            time.sleep(2)
            
            return True
        except Exception as e:
            self.logger.error(f"æŒ‰éˆ•é»æ“Šå¤±æ•—: {e}")
            return False

    def handle_recaptcha(self, element, timeout=30):
        """è™•ç†reCAPTCHAé©—è­‰"""
        try:
            self.logger.info("ğŸ¤– æª¢æ¸¬åˆ°reCAPTCHAï¼Œéœ€è¦æ‰‹å‹•å®Œæˆé©—è­‰")
            
            if self.headless:
                self.logger.warning("ç„¡é ­æ¨¡å¼ä¸‹ç„¡æ³•è™•ç†reCAPTCHAï¼Œå»ºè­°é—œé–‰headlessæ¨¡å¼")
                return False
            
            # é»æ“ŠreCAPTCHAè¤‡é¸æ¡†
            try:
                element.click()
                time.sleep(2)
            except:
                pass
            
            # ç­‰å¾…ç”¨æˆ¶æ‰‹å‹•å®Œæˆé©—è­‰
            self.logger.info(f"â° è«‹åœ¨ {timeout} ç§’å…§å®ŒæˆreCAPTCHAé©—è­‰...")
            
            start_time = time.time()
            while time.time() - start_time < timeout:
                try:
                    # æª¢æŸ¥é©—è­‰æ˜¯å¦å®Œæˆ
                    if not self.detect_verification_elements():
                        self.logger.info("âœ… reCAPTCHAé©—è­‰å®Œæˆ")
                        return True
                    time.sleep(2)
                except:
                    pass
            
            self.logger.warning("reCAPTCHAé©—è­‰è¶…æ™‚")
            return False
            
        except Exception as e:
            self.logger.error(f"è™•ç†reCAPTCHAå¤±æ•—: {e}")
            return False

    def handle_slider_verification(self, element):
        """è™•ç†æ»‘å‹•é©—è­‰"""
        try:
            from selenium.webdriver.common.action_chains import ActionChains
            
            self.logger.info("ğŸ¯ è™•ç†æ»‘å‹•é©—è­‰")
            
            # ç²å–æ»‘å¡Šçš„å°ºå¯¸å’Œä½ç½®
            slider_size = element.size
            slider_location = element.location
            
            # å‰µå»ºå‹•ä½œéˆ
            actions = ActionChains(self.driver)
            
            # é»æ“Šä¸¦æ‹–æ‹½æ»‘å¡Š
            actions.click_and_hold(element)
            
            # æ¨¡æ“¬äººé¡æ»‘å‹•è»Œè·¡
            for i in range(10):
                x_offset = (slider_size['width'] * 0.8) / 10
                actions.move_by_offset(x_offset, random.uniform(-2, 2))
                time.sleep(random.uniform(0.1, 0.3))
            
            actions.release().perform()
            time.sleep(3)
            
            return True
            
        except Exception as e:
            self.logger.error(f"æ»‘å‹•é©—è­‰å¤±æ•—: {e}")
            return False

    def handle_cloudflare(self, timeout=30):
        """è™•ç†CloudFlareé©—è­‰"""
        try:
            self.logger.info("â˜ï¸ æª¢æ¸¬åˆ°CloudFlareé©—è­‰ï¼Œç­‰å¾…è‡ªå‹•å®Œæˆ...")
            
            start_time = time.time()
            while time.time() - start_time < timeout:
                try:
                    # æª¢æŸ¥æ˜¯å¦é‚„åœ¨é©—è­‰é é¢
                    current_url = self.driver.current_url
                    if 'challenge' not in current_url.lower():
                        self.logger.info("âœ… CloudFlareé©—è­‰å®Œæˆ")
                        return True
                    time.sleep(2)
                except:
                    pass
            
            self.logger.warning("CloudFlareé©—è­‰è¶…æ™‚")
            return False
            
        except Exception as e:
            self.logger.error(f"è™•ç†CloudFlareå¤±æ•—: {e}")
            return False

    def load_chapter_list(self):
        """è¼‰å…¥ç« ç¯€åˆ—è¡¨"""
        try:
            if not Path(self.csv_file_path).exists():
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°CSVæª”æ¡ˆ: {self.csv_file_path}")
            
            df = pd.read_csv(self.csv_file_path, encoding='utf-8')
            df = df.dropna()
            
            chapters = []
            for _, row in df.iterrows():
                title = row['tablescraper-selected-row']
                url = row['tablescraper-selected-row href']
                if title and url:
                    chapters.append({
                        'title': str(title).strip(),
                        'url': str(url).strip()
                    })
            
            self.logger.info(f"è¼‰å…¥äº† {len(chapters)} å€‹ç« ç¯€")
            return chapters
        except Exception as e:
            self.logger.error(f"è¼‰å…¥CSVæª”æ¡ˆå¤±æ•—: {e}")
            return []

    def detect_pagination(self, title):
        """
        æª¢æ¸¬æ¨™é¡Œä¸­çš„åˆ†é ä¿¡æ¯
        è¿”å›: (current_page, total_pages) æˆ– None
        """
        for pattern in self.pagination_patterns:
            match = re.search(pattern, title)
            if match:
                current_page = int(match.group(1))
                total_pages = int(match.group(2))
                self.logger.debug(f"æª¢æ¸¬åˆ°åˆ†é : {current_page}/{total_pages}")
                return current_page, total_pages
        return None

    def construct_page_url(self, base_url, page_number):
        """
        æ ¹æ“šåŸºç¤URLå’Œé ç¢¼æ§‹é€ åˆ†é URL
        ä¾‹: https://www.novel543.com/0621496793/8096_1.html -> https://www.novel543.com/0621496793/8096_1_2.html
        """
        try:
            # è§£æURL
            parsed = urlparse(base_url)
            path_parts = parsed.path.split('/')
            
            # æ‰¾åˆ°æœ€å¾Œä¸€å€‹éƒ¨åˆ†ï¼ˆæª”æ¡ˆåï¼‰
            filename = path_parts[-1]
            
            # è™•ç†ä¸åŒçš„URLæ¨¡å¼
            if '.html' in filename:
                # ç§»é™¤.htmlå¾Œç¶´
                base_name = filename.replace('.html', '')
                
                # æ§‹é€ æ–°çš„æª”æ¡ˆå
                new_filename = f"{base_name}_{page_number}.html"
                
                # é‡æ–°æ§‹é€ URL
                path_parts[-1] = new_filename
                new_path = '/'.join(path_parts)
                
                new_url = urlunparse((
                    parsed.scheme,
                    parsed.netloc,
                    new_path,
                    parsed.params,
                    parsed.query,
                    parsed.fragment
                ))
                
                self.logger.debug(f"æ§‹é€ åˆ†é URL: {new_url}")
                return new_url
            else:
                self.logger.warning(f"ç„¡æ³•è™•ç†çš„URLæ ¼å¼: {base_url}")
                return None
                
        except Exception as e:
            self.logger.error(f"æ§‹é€ åˆ†é URLå¤±æ•—: {e}")
            return None

    def extract_content_from_page(self):
        """å¾ç•¶å‰é é¢æå–å…§å®¹"""
        try:
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # é‡å°Novel543çš„ç‰¹å®šé¸æ“‡å™¨
            content_selectors = [
                "#content",
                ".content", 
                ".novel-content",
                ".chapter-content",
                ".text-content",
                "div[class*='content']",
                "div[id*='content']",
                ".reading-content",
                "#chapterContent"
            ]
            
            content = ""
            title = ""
            
            # å…ˆå˜—è©¦ç²å–æ¨™é¡Œ
            title_selectors = ["h1", ".title", ".chapter-title", "h2", "h3"]
            for selector in title_selectors:
                try:
                    title_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if title_element:
                        title = title_element.text.strip()
                        break
                except:
                    continue
            
            # ç²å–å…§å®¹
            for selector in content_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        for element in elements:
                            text = element.text.strip()
                            if len(text) > len(content):
                                content = text
                        if content and len(content) > 100:  # ç¢ºä¿å…§å®¹è¶³å¤ é•·
                            break
                except:
                    continue
            
            # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
            if not content:
                try:
                    divs = self.driver.find_elements(By.TAG_NAME, "div")
                    if divs:
                        content = max(divs, key=lambda div: len(div.text)).text.strip()
                except:
                    pass
            
            return title, content
            
        except Exception as e:
            self.logger.error(f"æå–é é¢å…§å®¹å¤±æ•—: {e}")
            return "", ""

    def scrape_paginated_chapter(self, chapter_info):
        """çˆ¬å–åŒ…å«åˆ†é çš„å®Œæ•´ç« ç¯€"""
        try:
            base_url = chapter_info['url']
            chapter_title = chapter_info['title']
            
            self.logger.info(f"ğŸ” é–‹å§‹åˆ†æç« ç¯€: {chapter_title}")
            
            # è¨ªå•ç¬¬ä¸€é 
            self.driver.get(base_url)
            
            # ç­‰å¾…é é¢åŠ è¼‰
            if not self.wait_for_page_load():
                self.logger.warning("é é¢åŠ è¼‰å¯èƒ½ä¸å®Œæ•´")
            
            # è™•ç†é©—è­‰ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
            if not self.handle_verification():
                self.logger.warning(f"âš ï¸ é©—è­‰è™•ç†å¤±æ•—ï¼Œå˜—è©¦ç¹¼çºŒ: {chapter_title}")
            
            # é¡å¤–ç­‰å¾…ç¢ºä¿é é¢ç©©å®š
            time.sleep(random.uniform(2, 4))
            
            # ç²å–ç¬¬ä¸€é çš„æ¨™é¡Œå’Œå…§å®¹
            page_title, page_content = self.extract_content_from_page()
            
            if not page_content:
                self.logger.warning(f"âŒ ç„¡æ³•ç²å–ç¬¬ä¸€é å…§å®¹: {chapter_title}")
                return {
                    'title': chapter_title,
                    'url': base_url,
                    'content': '',
                    'status': 'no_content',
                    'pages': 0
                }
            
            # æª¢æ¸¬æ˜¯å¦æœ‰åˆ†é 
            pagination_info = self.detect_pagination(page_title)
            
            if pagination_info is None:
                # æ²’æœ‰åˆ†é ï¼Œç›´æ¥è¿”å›
                self.logger.info(f"âœ… å–®é ç« ç¯€: {chapter_title}")
                return {
                    'title': chapter_title,
                    'url': base_url,
                    'content': page_content,
                    'status': 'success',
                    'pages': 1
                }
            
            # æœ‰åˆ†é ï¼Œç²å–æ‰€æœ‰é é¢
            current_page, total_pages = pagination_info
            self.logger.info(f"ğŸ“„ æª¢æ¸¬åˆ°åˆ†é ç« ç¯€: {total_pages} é ")
            
            all_content = [page_content]  # ç¬¬ä¸€é å…§å®¹
            failed_pages = []
            
            # çˆ¬å–å‰©é¤˜é é¢
            for page_num in range(2, total_pages + 1):
                try:
                    page_url = self.construct_page_url(base_url, page_num)
                    if not page_url:
                        failed_pages.append(page_num)
                        continue
                    
                    self.logger.info(f"  ğŸ“– çˆ¬å–ç¬¬ {page_num}/{total_pages} é ...")
                    self.driver.get(page_url)
                    
                    # ç­‰å¾…é é¢åŠ è¼‰
                    self.wait_for_page_load(timeout=5)
                    
                    # è™•ç†å¯èƒ½çš„é©—è­‰
                    self.handle_verification(max_attempts=1, manual_timeout=min(10, self.verification_timeout))
                    
                    time.sleep(random.uniform(1, 3))
                    
                    _, content = self.extract_content_from_page()
                    
                    if content:
                        all_content.append(content)
                        self.logger.debug(f"    âœ… ç¬¬{page_num}é æˆåŠŸ (é•·åº¦: {len(content)})")
                    else:
                        failed_pages.append(page_num)
                        self.logger.warning(f"    âŒ ç¬¬{page_num}é å…§å®¹ç‚ºç©º")
                        
                except Exception as e:
                    failed_pages.append(page_num)
                    self.logger.error(f"    âŒ ç¬¬{page_num}é çˆ¬å–å¤±æ•—: {e}")
            
            # åˆä½µæ‰€æœ‰å…§å®¹
            combined_content = '\n\n'.join(all_content)
            
            success_pages = total_pages - len(failed_pages)
            status = 'success' if success_pages >= total_pages * 0.8 else 'partial_success'
            
            self.logger.info(f"ğŸ‰ ç« ç¯€å®Œæˆ: {success_pages}/{total_pages} é æˆåŠŸ")
            
            return {
                'title': chapter_title,
                'url': base_url,
                'content': combined_content,
                'status': status,
                'pages': success_pages,
                'total_pages': total_pages,
                'failed_pages': failed_pages
            }
            
        except Exception as e:
            self.logger.error(f"âŒ ç« ç¯€çˆ¬å–å¤±æ•— {chapter_info['title']}: {e}")
            return {
                'title': chapter_info['title'],
                'url': chapter_info['url'],
                'content': '',
                'status': 'error',
                'pages': 0,
                'error': str(e)
            }

    def save_chapter(self, chapter_data, chapter_num):
        """ä¿å­˜ç« ç¯€å…§å®¹"""
        try:
            safe_title = "".join(c for c in chapter_data['title'] if c.isalnum() or c in (' ', '-', '_', 'ï¼', 'ï¼Ÿ')).rstrip()
            filename = f"{chapter_num:03d}_{safe_title[:50]}.txt"
            filepath = Path(self.output_dir) / filename
            
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"æ¨™é¡Œ: {chapter_data['title']}\n")
                f.write(f"ç¶²å€: {chapter_data['url']}\n")
                f.write(f"ç‹€æ…‹: {chapter_data['status']}\n")
                f.write(f"ç« ç¯€ç·¨è™Ÿ: {chapter_num}\n")
                f.write(f"ç¸½é æ•¸: {chapter_data.get('total_pages', 1)}\n")
                f.write(f"æˆåŠŸé æ•¸: {chapter_data.get('pages', 1)}\n")
                if chapter_data.get('failed_pages'):
                    f.write(f"å¤±æ•—é æ•¸: {chapter_data['failed_pages']}\n")
                f.write("-" * 50 + "\n\n")
                f.write(chapter_data['content'])
            
            return str(filepath)
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç« ç¯€å¤±æ•—: {e}")
            return None

    def scrape_range(self, start_chapter=1, end_chapter=5, delay_range=(3, 6)):
        """çˆ¬å–æŒ‡å®šç¯„åœçš„ç« ç¯€"""
        if not self.setup_driver():
            return []
        
        try:
            chapters = self.load_chapter_list()
            if not chapters:
                return []
            
            total_chapters = len(chapters)
            end_chapter = min(end_chapter, total_chapters)
            
            selected_chapters = chapters[start_chapter-1:end_chapter]
            self.logger.info(f"ğŸš€ é–‹å§‹çˆ¬å–ç¬¬ {start_chapter} åˆ°ç¬¬ {end_chapter} ç« ï¼Œå…± {len(selected_chapters)} ç« ")
            
            results = []
            success_count = 0
            total_pages = 0
            
            for i, chapter_info in enumerate(selected_chapters, start_chapter):
                result = self.scrape_paginated_chapter(chapter_info)
                
                if result['status'] in ['success', 'partial_success']:
                    filepath = self.save_chapter(result, i)
                    if filepath:
                        result['saved_path'] = filepath
                        success_count += 1
                        total_pages += result.get('pages', 0)
                
                results.append(result)
                
                # é€²åº¦å ±å‘Š
                progress = f"[{i-start_chapter+1}/{len(selected_chapters)}]"
                pages_info = f"(å…±çˆ¬å– {total_pages} é )"
                self.logger.info(f"{progress} é€²åº¦æ›´æ–° - æˆåŠŸ: {success_count} {pages_info}")
                
                # å»¶é²
                if i < end_chapter:
                    delay = random.uniform(delay_range[0], delay_range[1])
                    time.sleep(delay)
            
            # ä¿å­˜æ‘˜è¦
            self.save_summary(results, start_chapter, end_chapter, total_pages)
            
            failed_count = len(results) - success_count
            self.logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±æ•—: {failed_count}, ç¸½é æ•¸: {total_pages}")
            
            return results
            
        finally:
            if self.driver:
                self.driver.quit()

    def save_summary(self, results, start_chapter, end_chapter, total_pages):
        """ä¿å­˜çˆ¬å–çµæœæ‘˜è¦"""
        summary = {
            'range': f"{start_chapter}-{end_chapter}",
            'total_chapters': len(results),
            'successful_chapters': sum(1 for r in results if r['status'] in ['success', 'partial_success']),
            'total_pages_scraped': total_pages,
            'average_pages_per_chapter': total_pages / len(results) if results else 0,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'chapter_details': [
                {
                    'title': r['title'],
                    'status': r['status'],
                    'pages': r.get('pages', 0),
                    'total_pages': r.get('total_pages', 1)
                } for r in results
            ]
        }
        
        summary_path = Path(self.output_dir) / f'paginated_summary_{start_chapter}-{end_chapter}.json'
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

def main():
    parser = argparse.ArgumentParser(description='åˆ†é å°èªªçˆ¬èŸ² - æ”¯æŒé©—è­‰è™•ç†')
    parser.add_argument('csv_file', help='CSVæª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--start', '-s', type=int, default=1, help='é–‹å§‹ç« ç¯€')
    parser.add_argument('--end', '-e', type=int, default=5, help='çµæŸç« ç¯€')
    parser.add_argument('--output', '-o', default='paginated_novels', help='è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--delay', '-d', default='3-6', help='å»¶é²æ™‚é–“ç¯„åœ')
    parser.add_argument('--headless', action='store_true', help='ç„¡é ­æ¨¡å¼')
    parser.add_argument('--test', action='store_true', help='æ¸¬è©¦æ¨¡å¼ï¼ˆå‰3ç« ï¼‰')
    parser.add_argument('--no-verify', action='store_true', help='é—œé–‰è‡ªå‹•é©—è­‰è™•ç†')
    parser.add_argument('--verify-timeout', type=int, default=30, help='æ‰‹å‹•é©—è­‰è¶…æ™‚æ™‚é–“ï¼ˆç§’ï¼‰')
    parser.add_argument('--custom-verify', help='è‡ªå®šç¾©é©—è­‰å…ƒç´ é¸æ“‡å™¨ï¼ˆCSSé¸æ“‡å™¨ï¼‰')
    
    args = parser.parse_args()
    
    # è§£æå»¶é²ç¯„åœ
    try:
        if '-' in args.delay:
            min_delay, max_delay = map(float, args.delay.split('-'))
            delay_range = (min_delay, max_delay)
        else:
            delay = float(args.delay)
            delay_range = (delay, delay + 2)
    except:
        delay_range = (3, 6)
    
    # æ¸¬è©¦æ¨¡å¼
    if args.test:
        start_chapter, end_chapter = 1, 3
        print("ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šçˆ¬å–å‰3ç« ")
    else:
        start_chapter, end_chapter = args.start, args.end
    
    print(f"ğŸ” åˆ†é å°èªªçˆ¬èŸ²å•Ÿå‹•")
    print(f"ğŸ“– çˆ¬å–ç¯„åœ: ç¬¬{start_chapter}-{end_chapter}ç« ")
    print(f"â±ï¸  å»¶é²è¨­ç½®: {delay_range[0]}-{delay_range[1]}ç§’")
    print(f"ğŸ” è‡ªå‹•é©—è­‰: {'é—œé–‰' if args.no_verify else 'é–‹å•Ÿ'}")
    if not args.no_verify:
        print(f"â° é©—è­‰è¶…æ™‚: {args.verify_timeout}ç§’")
    
    scraper = PaginatedNovelScraper(
        csv_file_path=args.csv_file,
        output_dir=args.output,
        headless=args.headless,
        auto_verify=not args.no_verify
    )
    
    # è¨­ç½®é©—è­‰è¶…æ™‚æ™‚é–“
    scraper.verification_timeout = args.verify_timeout
    
    # æ·»åŠ è‡ªå®šç¾©é©—è­‰é¸æ“‡å™¨
    if args.custom_verify:
        scraper.verification_selectors.append(args.custom_verify)
        print(f"ğŸ”§ æ·»åŠ è‡ªå®šç¾©é©—è­‰é¸æ“‡å™¨: {args.custom_verify}")
    
    results = scraper.scrape_range(start_chapter, end_chapter, delay_range)
    
    if results:
        success_count = sum(1 for r in results if r['status'] in ['success', 'partial_success'])
        total_pages = sum(r.get('pages', 0) for r in results)
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"   æˆåŠŸç« ç¯€: {success_count}/{len(results)}")
        print(f"   ç¸½é æ•¸: {total_pages}")
        print(f"   è¼¸å‡ºç›®éŒ„: {args.output}")
        
        # å¦‚æœæœ‰é©—è­‰ç›¸é—œçš„å•é¡Œï¼Œçµ¦å‡ºå»ºè­°
        if success_count < len(results) * 0.8:
            print(f"\nğŸ’¡ æˆåŠŸç‡è¼ƒä½ï¼Œå»ºè­°ï¼š")
            print(f"   - å¦‚æœé‡åˆ°é©—è­‰å•é¡Œï¼Œå¯ä»¥é—œé–‰ç„¡é ­æ¨¡å¼: --no-headless")
            print(f"   - å¦‚æœé©—è­‰è¶…æ™‚ï¼Œå¯ä»¥å¢åŠ è¶…æ™‚æ™‚é–“: --verify-timeout 60")
            print(f"   - å¦‚æœé©—è­‰è™•ç†æœ‰å•é¡Œï¼Œå¯ä»¥é—œé–‰è‡ªå‹•é©—è­‰: --no-verify")

if __name__ == "__main__":
    main()
