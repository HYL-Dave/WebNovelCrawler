#!/usr/bin/env python3
"""
çˆ¬èŸ²ç›£æ§å’Œçµ±è¨ˆè…³æœ¬
"""

import os
import re
import json
import time
import argparse
from datetime import datetime
from collections import defaultdict


class CrawlerMonitor:
    def __init__(self, output_dir):
        self.output_dir = output_dir
        self.stats = {
            'total_files': 0,
            'successful_files': 0,
            'empty_files': 0,
            'error_files': 0,
            'total_chars': 0,
            'avg_chars_per_chapter': 0,
            'chapters_with_images': 0,
            'chapters_with_ocr': 0,
            'decode_success_rate': 0,
            'common_errors': defaultdict(int),
            'processing_times': [],
            'chapter_lengths': [],
            'missing_chapters': [],
            'duplicate_chapters': defaultdict(int)
        }

    def analyze_output(self):
        """åˆ†æè¼¸å‡ºç›®éŒ„"""
        print(f"åˆ†æç›®éŒ„: {self.output_dir}")

        if not os.path.exists(self.output_dir):
            print(f"éŒ¯èª¤: ç›®éŒ„ {self.output_dir} ä¸å­˜åœ¨")
            return

        files = os.listdir(self.output_dir)

        # åˆ†ææ–‡ä»¶
        for filename in files:
            filepath = os.path.join(self.output_dir, filename)

            if filename.endswith('.txt'):
                self.analyze_text_file(filepath, filename)
            elif filename.endswith('.log'):
                self.analyze_log_file(filepath, filename)
            elif filename.endswith('.png'):
                self.stats['chapters_with_images'] += 1

        self.calculate_statistics()
        self.generate_report()

    def analyze_text_file(self, filepath, filename):
        """åˆ†ææ–‡æœ¬æ–‡ä»¶"""
        self.stats['total_files'] += 1

        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read().strip()

            if not content:
                self.stats['empty_files'] += 1
                return

            if filename.startswith('error_'):
                self.stats['error_files'] += 1
                return

            # æˆåŠŸçš„æ–‡ä»¶
            self.stats['successful_files'] += 1
            char_count = len(content)
            self.stats['total_chars'] += char_count
            self.stats['chapter_lengths'].append(char_count)

            # æª¢æŸ¥å…§å®¹è³ªé‡
            self.analyze_content_quality(content, filename)

            # æå–ç« ç¯€è™Ÿ
            chapter_match = re.search(r'chapter_(\d+)', filename)
            if chapter_match:
                chapter_num = int(chapter_match.group(1))
                self.stats['duplicate_chapters'][chapter_num] += 1

        except Exception as e:
            print(f"åˆ†ææ–‡ä»¶å¤±æ•— {filename}: {e}")
            self.stats['common_errors'][str(e)] += 1

    def analyze_content_quality(self, content, filename):
        """åˆ†æå…§å®¹è³ªé‡"""
        # æª¢æŸ¥æ˜¯å¦åŒ…å«OCRå…§å®¹
        if '=== åœ–ç‰‡æ–‡å­— ===' in content or '=== Canvasæ–‡å­— ===' in content:
            self.stats['chapters_with_ocr'] += 1

        # æª¢æŸ¥æ¨™é»ç¬¦è™Ÿ
        punctuation_count = len(re.findall(r'[ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]', content))
        if punctuation_count > 0:
            self.stats['decode_success_rate'] += 1

        # æª¢æŸ¥æ˜¯å¦æœ‰å»£å‘Šæ®˜ç•™
        ad_indicators = ['é›œæ›¸å±‹', 'zashuwu', 'è¨˜éƒµä»¶', 'æœ€æ–°ç« ç¯€']
        has_ads = any(ad in content for ad in ad_indicators)
        if has_ads:
            self.stats['common_errors']['å»£å‘Šæ¸…ç†ä¸å®Œæ•´'] += 1

        # æª¢æŸ¥ç« ç¯€å®Œæ•´æ€§
        if len(content) < 500:
            self.stats['common_errors']['å…§å®¹éçŸ­'] += 1
        elif len(content) > 50000:
            self.stats['common_errors']['å…§å®¹éé•·'] += 1

    def analyze_log_file(self, filepath, filename):
        """åˆ†ææ—¥èªŒæ–‡ä»¶"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                lines = f.readlines()

            for line in lines:
                line = line.strip()
                if 'å¤±æ•—' in line or 'éŒ¯èª¤' in line:
                    # æå–éŒ¯èª¤é¡å‹
                    if 'timeout' in line.lower():
                        self.stats['common_errors']['è¶…æ™‚'] += 1
                    elif 'detection' in line.lower() or 'æª¢æ¸¬' in line:
                        self.stats['common_errors']['åçˆ¬æª¢æ¸¬'] += 1
                    elif 'decode' in line.lower() or 'è§£ç¢¼' in line:
                        self.stats['common_errors']['è§£ç¢¼å¤±æ•—'] += 1
                    else:
                        self.stats['common_errors']['å…¶ä»–éŒ¯èª¤'] += 1

        except Exception as e:
            print(f"åˆ†ææ—¥èªŒå¤±æ•— {filename}: {e}")

    def calculate_statistics(self):
        """è¨ˆç®—çµ±è¨ˆæ•¸æ“š"""
        if self.stats['successful_files'] > 0:
            self.stats['avg_chars_per_chapter'] = int(
                self.stats['total_chars'] / self.stats['successful_files']
            )

        if self.stats['total_files'] > 0:
            self.stats['decode_success_rate'] = int(
                (self.stats['decode_success_rate'] / self.stats['total_files']) * 100
            )

        # æ‰¾å‡ºç¼ºå¤±çš„ç« ç¯€
        if self.stats['duplicate_chapters']:
            chapter_nums = list(self.stats['duplicate_chapters'].keys())
            min_chapter = min(chapter_nums)
            max_chapter = max(chapter_nums)

            for i in range(min_chapter, max_chapter + 1):
                if i not in self.stats['duplicate_chapters']:
                    self.stats['missing_chapters'].append(i)

    def generate_report(self):
        """ç”Ÿæˆå ±å‘Š"""
        print(f"\n{'=' * 60}")
        print(f"çˆ¬èŸ²åˆ†æå ±å‘Š - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'=' * 60}")

        # åŸºæœ¬çµ±è¨ˆ
        print(f"\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"  ç¸½æ–‡ä»¶æ•¸: {self.stats['total_files']}")
        print(f"  æˆåŠŸæ–‡ä»¶: {self.stats['successful_files']}")
        print(f"  ç©ºæ–‡ä»¶: {self.stats['empty_files']}")
        print(f"  éŒ¯èª¤æ–‡ä»¶: {self.stats['error_files']}")
        print(f"  æˆåŠŸç‡: {self.get_success_rate():.1f}%")

        # å…§å®¹çµ±è¨ˆ
        print(f"\nğŸ“ å…§å®¹çµ±è¨ˆ:")
        print(f"  ç¸½å­—ç¬¦æ•¸: {self.stats['total_chars']:,}")
        print(f"  å¹³å‡æ¯ç« å­—ç¬¦æ•¸: {self.stats['avg_chars_per_chapter']:,}")
        print(f"  è§£ç¢¼æˆåŠŸç‡: {self.stats['decode_success_rate']}%")
        print(f"  åŒ…å«OCRå…§å®¹çš„ç« ç¯€: {self.stats['chapters_with_ocr']}")

        # è³ªé‡åˆ†æ
        if self.stats['chapter_lengths']:
            lengths = self.stats['chapter_lengths']
            print(f"\nğŸ“ˆ ç« ç¯€é•·åº¦åˆ†å¸ƒ:")
            print(f"  æœ€çŸ­: {min(lengths):,} å­—ç¬¦")
            print(f"  æœ€é•·: {max(lengths):,} å­—ç¬¦")
            print(f"  ä¸­ä½æ•¸: {self.get_median(lengths):,} å­—ç¬¦")

        # å•é¡Œåˆ†æ
        if self.stats['common_errors']:
            print(f"\nâš ï¸  å¸¸è¦‹å•é¡Œ:")
            for error, count in sorted(self.stats['common_errors'].items(),
                                       key=lambda x: x[1], reverse=True):
                print(f"  {error}: {count} æ¬¡")

        # ç¼ºå¤±ç« ç¯€
        if self.stats['missing_chapters']:
            print(f"\nâŒ ç¼ºå¤±ç« ç¯€: {self.stats['missing_chapters']}")

        # é‡è¤‡ç« ç¯€
        duplicates = {k: v for k, v in self.stats['duplicate_chapters'].items() if v > 1}
        if duplicates:
            print(f"\nğŸ”„ é‡è¤‡ç« ç¯€: {duplicates}")

        # å»ºè­°
        self.generate_recommendations()

        # ä¿å­˜è©³ç´°å ±å‘Š
        self.save_detailed_report()

    def get_success_rate(self):
        """è¨ˆç®—æˆåŠŸç‡"""
        if self.stats['total_files'] == 0:
            return 0
        return (self.stats['successful_files'] / self.stats['total_files']) * 100

    def get_median(self, numbers):
        """è¨ˆç®—ä¸­ä½æ•¸"""
        sorted_numbers = sorted(numbers)
        n = len(sorted_numbers)
        if n % 2 == 0:
            return (sorted_numbers[n // 2 - 1] + sorted_numbers[n // 2]) // 2
        else:
            return sorted_numbers[n // 2]

    def generate_recommendations(self):
        """ç”Ÿæˆæ”¹é€²å»ºè­°"""
        print(f"\nğŸ’¡ æ”¹é€²å»ºè­°:")

        success_rate = self.get_success_rate()

        if success_rate < 50:
            print("  - æˆåŠŸç‡è¼ƒä½ï¼Œå»ºè­°æª¢æŸ¥ç¶²ç«™åçˆ¬æ©Ÿåˆ¶")
            print("  - è€ƒæ…®å¢åŠ å»¶é²æ™‚é–“æˆ–ä½¿ç”¨ä»£ç†")

        if self.stats['decode_success_rate'] < 80:
            print("  - è§£ç¢¼æˆåŠŸç‡ä¸ç†æƒ³ï¼Œæª¢æŸ¥è§£ç¢¼å™¨é…ç½®")
            print("  - é©—è­‰æ¨™é»ç¬¦è™Ÿæ˜ å°„æ˜¯å¦å®Œæ•´")

        if self.stats['common_errors'].get('å…§å®¹éçŸ­', 0) > 5:
            print("  - å¤šå€‹ç« ç¯€å…§å®¹éçŸ­ï¼Œå¯èƒ½å­˜åœ¨æå–å•é¡Œ")
            print("  - å»ºè­°å•Ÿç”¨OCRåŠŸèƒ½æˆ–æª¢æŸ¥é¸æ“‡å™¨")

        if self.stats['missing_chapters']:
            print(f"  - æœ‰ {len(self.stats['missing_chapters'])} å€‹ç« ç¯€ç¼ºå¤±")
            print("  - å»ºè­°é‡æ–°çˆ¬å–ç¼ºå¤±çš„ç« ç¯€")

        if self.stats['chapters_with_ocr'] == 0 and self.stats['successful_files'] > 0:
            print("  - æœªæª¢æ¸¬åˆ°OCRå…§å®¹ï¼Œå¦‚æœç¶²ç«™ä½¿ç”¨åœ–ç‰‡æ–‡å­—ï¼Œå»ºè­°å•Ÿç”¨OCR")

    def save_detailed_report(self):
        """ä¿å­˜è©³ç´°å ±å‘Š"""
        report_file = os.path.join(self.output_dir, 'analysis_report.json')

        # æº–å‚™å¯åºåˆ—åŒ–çš„æ•¸æ“š
        serializable_stats = dict(self.stats)
        serializable_stats['common_errors'] = dict(serializable_stats['common_errors'])
        serializable_stats['duplicate_chapters'] = dict(serializable_stats['duplicate_chapters'])
        serializable_stats['analysis_time'] = datetime.now().isoformat()

        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_stats, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“„ è©³ç´°å ±å‘Šå·²ä¿å­˜: {report_file}")
        except Exception as e:
            print(f"ä¿å­˜å ±å‘Šå¤±æ•—: {e}")

    def fix_missing_chapters(self, csv_file):
        """ç”Ÿæˆä¿®å¾©ç¼ºå¤±ç« ç¯€çš„å‘½ä»¤"""
        if not self.stats['missing_chapters']:
            print("æ²’æœ‰ç¼ºå¤±çš„ç« ç¯€éœ€è¦ä¿®å¾©")
            return

        print(f"\nğŸ”§ ä¿®å¾©ç¼ºå¤±ç« ç¯€çš„å»ºè­°å‘½ä»¤:")

        for chapter in self.stats['missing_chapters']:
            print(f"  # ä¿®å¾©ç¬¬ {chapter} ç« ")
            print(f"  python comprehensive_novel_crawler.py \\")
            print(f"    --csv {csv_file} \\")
            print(f"    --start {chapter - 1} \\")
            print(f"    --end {chapter} \\")
            print(f"    --output {self.output_dir}")
            print()


def main():
    parser = argparse.ArgumentParser(description='çˆ¬èŸ²çµæœç›£æ§å’Œåˆ†æ')
    parser.add_argument('--output-dir', type=str, required=True,
                        help='è¦åˆ†æçš„è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--csv', type=str, default='m1.csv',
                        help='åŸå§‹CSVæ–‡ä»¶ï¼ˆç”¨æ–¼ç”Ÿæˆä¿®å¾©å‘½ä»¤ï¼‰')
    parser.add_argument('--fix', action='store_true',
                        help='ç”Ÿæˆä¿®å¾©ç¼ºå¤±ç« ç¯€çš„å‘½ä»¤')

    args = parser.parse_args()

    monitor = CrawlerMonitor(args.output_dir)
    monitor.analyze_output()

    if args.fix:
        monitor.fix_missing_chapters(args.csv)


if __name__ == "__main__":
    main()