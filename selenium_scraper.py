#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°èªªçˆ¬èŸ² - Seleniumç‰ˆæœ¬ (æ”¯æŒå‘½ä»¤è¡Œåƒæ•¸)
https://czbooks.net/
ä½¿ç”¨æ–¹æ³•ï¼š
    python scraper.py input.csv --start 1 --end 10 --delay 3 --output novels
    python scraper.py input.csv --all --headless
"""

import argparse
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, WebDriverException
import time
import os
import json
import logging
import random
import sys
from pathlib import Path


class SeleniumNovelScraper:
    def __init__(self, csv_file_path, output_dir="novel_chapters", headless=False, user_agent=None):
        """
        åˆå§‹åŒ–Seleniumçˆ¬èŸ²
        """
        self.csv_file_path = csv_file_path
        self.output_dir = output_dir
        self.headless = headless
        self.user_agent = user_agent or 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        self.driver = None

        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        Path(output_dir).mkdir(parents=True, exist_ok=True)

        # è¨­ç½®æ—¥èªŒ
        self.setup_logging()

    def setup_logging(self):
        """è¨­ç½®æ—¥èªŒç³»çµ±"""
        log_file = Path(self.output_dir) / 'scraping.log'

        # æ¸…é™¤ä¹‹å‰çš„handlers
        logging.getLogger().handlers.clear()

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)

    def setup_driver(self):
        """è¨­ç½®Chromeç€è¦½å™¨é©…å‹•"""
        try:
            chrome_options = Options()

            # åŸºæœ¬è¨­ç½®
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)

            # è¨­ç½®ç”¨æˆ¶ä»£ç†
            chrome_options.add_argument(f'--user-agent={self.user_agent}')

            # ç„¡é ­æ¨¡å¼
            if self.headless:
                chrome_options.add_argument('--headless')
                self.logger.info("é‹è¡Œåœ¨ç„¡é ­æ¨¡å¼")

            # å…¶ä»–å„ªåŒ–è¨­ç½®
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--no-first-run')
            chrome_options.add_argument('--disable-default-apps')

            # åˆå§‹åŒ–ç€è¦½å™¨
            try:
                # å˜—è©¦ä½¿ç”¨webdriver-managerè‡ªå‹•ç®¡ç†ChromeDriver
                from webdriver_manager.chrome import ChromeDriverManager
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
            except ImportError:
                # å¦‚æœæ²’æœ‰webdriver-managerï¼Œä½¿ç”¨ç³»çµ±PATHä¸­çš„chromedriver
                self.driver = webdriver.Chrome(options=chrome_options)

            # éš±è—è‡ªå‹•åŒ–ç‰¹å¾µ
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            self.logger.info("ç€è¦½å™¨é©…å‹•è¨­ç½®æˆåŠŸ")
            return True

        except Exception as e:
            self.logger.error(f"ç€è¦½å™¨é©…å‹•è¨­ç½®å¤±æ•—: {e}")
            self.logger.error("è«‹ç¢ºä¿å·²å®‰è£Chromeç€è¦½å™¨å’ŒChromeDriver")
            self.logger.error("æˆ–é‹è¡Œ: pip install webdriver-manager")
            return False

    def load_chapter_list(self):
        """è¼‰å…¥ç« ç¯€åˆ—è¡¨"""
        try:
            if not Path(self.csv_file_path).exists():
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°CSVæª”æ¡ˆ: {self.csv_file_path}")

            df = pd.read_csv(self.csv_file_path, encoding='utf-8')
            df = df.dropna()

            chapters = []
            for _, row in df.iterrows():
                title = row['tablescraper-selected-row']
                url = row['tablescraper-selected-row href']
                if title and url:
                    chapters.append({
                        'title': str(title).strip(),
                        'url': str(url).strip()
                    })

            self.logger.info(f"è¼‰å…¥äº† {len(chapters)} å€‹ç« ç¯€")
            return chapters
        except Exception as e:
            self.logger.error(f"è¼‰å…¥CSVæª”æ¡ˆå¤±æ•—: {e}")
            return []

    def human_like_delay(self, min_seconds=2, max_seconds=5):
        """æ¨¡æ“¬äººé¡é–±è®€çš„å»¶é²"""
        delay = random.uniform(min_seconds, max_seconds)
        self.logger.debug(f"ç­‰å¾… {delay:.1f} ç§’...")
        time.sleep(delay)

    def extract_content_selenium(self):
        """ä½¿ç”¨Seleniumæå–é é¢å…§å®¹"""
        try:
            # ç­‰å¾…é é¢åŠ è¼‰å®Œæˆ
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # å˜—è©¦å¤šå€‹å¯èƒ½çš„å…§å®¹é¸æ“‡å™¨
            content_selectors = [
                "div[class*='content']",
                "div[id*='content']",
                ".chapter-content",
                ".novel-content",
                ".text-content",
                "article",
                ".main-text",
                "div[class*='text']",
                ".reading-content",
                "#chapterContent"
            ]

            content = ""
            for selector in content_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        for element in elements:
                            text = element.text.strip()
                            if len(text) > len(content):
                                content = text
                        if content:
                            self.logger.debug(f"ä½¿ç”¨é¸æ“‡å™¨æ‰¾åˆ°å…§å®¹: {selector}")
                            break
                except:
                    continue

            # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œå˜—è©¦æ‰€æœ‰divå…ƒç´ 
            if not content:
                try:
                    divs = self.driver.find_elements(By.TAG_NAME, "div")
                    if divs:
                        content = max(divs, key=lambda div: len(div.text)).text.strip()
                        self.logger.debug("ä½¿ç”¨æœ€é•·divå…ƒç´ ç²å–å…§å®¹")
                except:
                    pass

            return content

        except TimeoutException:
            self.logger.warning("é é¢åŠ è¼‰è¶…æ™‚")
            return ""
        except Exception as e:
            self.logger.error(f"æå–å…§å®¹å¤±æ•—: {e}")
            return ""

    def scrape_chapter(self, chapter_info):
        """çˆ¬å–å–®å€‹ç« ç¯€"""
        try:
            url = chapter_info['url']
            title = chapter_info['title']

            self.logger.info(f"æ­£åœ¨çˆ¬å–: {title}")

            # è¨ªå•é é¢
            self.driver.get(url)

            # æ¨¡æ“¬äººé¡è¡Œç‚º - éš¨æ©Ÿæ»¾å‹•
            self.driver.execute_script("window.scrollTo(0, Math.floor(Math.random() * 1000));")

            # ç­‰å¾…ä¸€ä¸‹è®“é é¢å®Œå…¨åŠ è¼‰
            self.human_like_delay(1, 3)

            # æå–å…§å®¹
            content = self.extract_content_selenium()

            if content and len(content) > 50:  # ç¢ºä¿å…§å®¹ä¸æ˜¯å¤ªçŸ­
                self.logger.info(f"âœ“ æˆåŠŸ: {title} (å…§å®¹é•·åº¦: {len(content)})")
                return {
                    'title': title,
                    'url': url,
                    'content': content,
                    'status': 'success'
                }
            else:
                self.logger.warning(f"âœ— å…§å®¹å¤ªçŸ­æˆ–ç‚ºç©º: {title}")
                # ä¿å­˜é é¢æºç¢¼ç”¨æ–¼èª¿è©¦
                debug_filename = f"debug_{title[:30].replace('/', '_').replace('?', '').replace(':', '')}.html"
                debug_path = Path(self.output_dir) / debug_filename
                with open(debug_path, 'w', encoding='utf-8') as f:
                    f.write(self.driver.page_source)
                return {
                    'title': title,
                    'url': url,
                    'content': content,
                    'status': 'no_content',
                    'debug_file': str(debug_path)
                }

        except Exception as e:
            self.logger.error(f"âœ— çˆ¬å–å¤±æ•— {chapter_info['title']}: {e}")
            return {
                'title': chapter_info['title'],
                'url': chapter_info['url'],
                'content': '',
                'status': 'error',
                'error': str(e)
            }

    def save_chapter(self, chapter_data, chapter_num):
        """ä¿å­˜ç« ç¯€å…§å®¹"""
        try:
            safe_title = "".join(
                c for c in chapter_data['title'] if c.isalnum() or c in (' ', '-', '_', 'ï¼', 'ï¼Ÿ')).rstrip()
            filename = f"{chapter_num:03d}_{safe_title[:50]}.txt"  # é™åˆ¶æª”åé•·åº¦
            filepath = Path(self.output_dir) / filename

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"æ¨™é¡Œ: {chapter_data['title']}\n")
                f.write(f"ç¶²å€: {chapter_data['url']}\n")
                f.write(f"ç‹€æ…‹: {chapter_data['status']}\n")
                f.write(f"ç« ç¯€ç·¨è™Ÿ: {chapter_num}\n")
                f.write("-" * 50 + "\n\n")
                f.write(chapter_data['content'])

            return str(filepath)
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç« ç¯€å¤±æ•—: {e}")
            return None

    def scrape_range(self, start_chapter=1, end_chapter=5, delay_range=(3, 6)):
        """
        çˆ¬å–æŒ‡å®šç¯„åœçš„ç« ç¯€
        """
        if not self.setup_driver():
            return []

        try:
            chapters = self.load_chapter_list()
            if not chapters:
                self.logger.error("æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆç« ç¯€")
                return []

            # èª¿æ•´ç« ç¯€ç¯„åœ
            total_chapters = len(chapters)
            end_chapter = min(end_chapter, total_chapters)

            if start_chapter > total_chapters:
                self.logger.error(f"é–‹å§‹ç« ç¯€ {start_chapter} è¶…éç¸½ç« ç¯€æ•¸ {total_chapters}")
                return []

            selected_chapters = chapters[start_chapter - 1:end_chapter]
            self.logger.info(f"å°‡çˆ¬å–ç¬¬ {start_chapter} åˆ°ç¬¬ {end_chapter} ç« ï¼Œå…± {len(selected_chapters)} ç« ")

            results = []
            success_count = 0

            for i, chapter_info in enumerate(selected_chapters, start_chapter):
                result = self.scrape_chapter(chapter_info)

                if result['status'] == 'success':
                    filepath = self.save_chapter(result, i)
                    if filepath:
                        result['saved_path'] = filepath
                        success_count += 1

                results.append(result)

                # é€²åº¦å ±å‘Š
                progress = f"[{i - start_chapter + 1}/{len(selected_chapters)}]"
                self.logger.info(f"{progress} é€²åº¦æ›´æ–° - æˆåŠŸ: {success_count}")

                # äººé¡åŒ–å»¶é²
                if i < end_chapter:
                    self.human_like_delay(delay_range[0], delay_range[1])

            # ä¿å­˜æ‘˜è¦
            self.save_summary(results, start_chapter, end_chapter)

            # æœ€çµ‚çµ±è¨ˆ
            failed_count = len(results) - success_count
            self.logger.info(f"ğŸ‰ å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±æ•—: {failed_count}")

            return results

        finally:
            if self.driver:
                self.driver.quit()
                self.logger.info("ç€è¦½å™¨å·²é—œé–‰")

    def save_summary(self, results, start_chapter, end_chapter):
        """ä¿å­˜çˆ¬å–çµæœæ‘˜è¦"""
        summary = {
            'range': f"{start_chapter}-{end_chapter}",
            'total_attempted': len(results),
            'successful': sum(1 for r in results if r['status'] == 'success'),
            'failed': sum(1 for r in results if r['status'] != 'success'),
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'results': results
        }

        summary_path = Path(self.output_dir) / f'summary_{start_chapter}-{end_chapter}.json'
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)

        self.logger.info(f"ğŸ“Š çµæœæ‘˜è¦å·²ä¿å­˜: {summary_path}")


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(
        description='å°èªªçˆ¬èŸ² - Seleniumç‰ˆæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
  %(prog)s input.csv --start 1 --end 10
  %(prog)s input.csv --all --headless --delay 5
  %(prog)s input.csv -s 50 -e 100 -o my_novels --delay 3-8
        """
    )

    # å¿…éœ€åƒæ•¸
    parser.add_argument('csv_file', help='CSVæª”æ¡ˆè·¯å¾‘')

    # ç« ç¯€ç¯„åœ
    range_group = parser.add_mutually_exclusive_group(required=True)
    range_group.add_argument('--start', '-s', type=int, metavar='N',
                             help='é–‹å§‹ç« ç¯€ç·¨è™Ÿ')
    range_group.add_argument('--all', action='store_true',
                             help='çˆ¬å–æ‰€æœ‰ç« ç¯€')

    parser.add_argument('--end', '-e', type=int, metavar='N',
                        help='çµæŸç« ç¯€ç·¨è™Ÿ (èˆ‡--startä¸€èµ·ä½¿ç”¨)')

    # åŸºæœ¬è¨­ç½®
    parser.add_argument('--output', '-o', default='novel_chapters',
                        help='è¼¸å‡ºç›®éŒ„ (é è¨­: novel_chapters)')
    parser.add_argument('--delay', '-d', default='3-6',
                        help='å»¶é²æ™‚é–“ç¯„åœï¼Œç§’ (é è¨­: 3-6)')

    # ç€è¦½å™¨è¨­ç½®
    parser.add_argument('--headless', action='store_true',
                        help='ç„¡é ­æ¨¡å¼é‹è¡Œï¼ˆä¸é¡¯ç¤ºç€è¦½å™¨çª—å£ï¼‰')
    parser.add_argument('--user-agent', '-ua',
                        help='è‡ªå®šç¾©User-Agent')

    # å…¶ä»–é¸é …
    parser.add_argument('--verbose', '-v', action='store_true',
                        help='é¡¯ç¤ºè©³ç´°æ—¥èªŒ')
    parser.add_argument('--test', '-t', action='store_true',
                        help='æ¸¬è©¦æ¨¡å¼ï¼šåªçˆ¬å–å‰3ç« ')

    return parser.parse_args()


def parse_delay_range(delay_str):
    """è§£æå»¶é²æ™‚é–“ç¯„åœ"""
    try:
        if '-' in delay_str:
            min_delay, max_delay = map(float, delay_str.split('-'))
            return (min_delay, max_delay)
        else:
            delay = float(delay_str)
            return (delay, delay + 2)
    except ValueError:
        return (3, 6)  # é è¨­å€¼


def main():
    """ä¸»å‡½æ•¸"""
    args = parse_arguments()

    # è¨­ç½®æ—¥èªŒç´šåˆ¥
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # è§£æå»¶é²ç¯„åœ
    delay_range = parse_delay_range(args.delay)

    # æ±ºå®šç« ç¯€ç¯„åœ
    if args.test:
        start_chapter, end_chapter = 1, 3
        print("ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šçˆ¬å–å‰3ç« ")
    elif args.all:
        start_chapter, end_chapter = 1, float('inf')
        print("ğŸ“š çˆ¬å–æ‰€æœ‰ç« ç¯€")
    else:
        start_chapter = args.start
        end_chapter = args.end or args.start
        print(f"ğŸ“– çˆ¬å–ç¬¬ {start_chapter} åˆ°ç¬¬ {end_chapter} ç« ")

    # å‰µå»ºçˆ¬èŸ²å¯¦ä¾‹
    scraper = SeleniumNovelScraper(
        csv_file_path=args.csv_file,
        output_dir=args.output,
        headless=args.headless,
        user_agent=args.user_agent
    )

    # å¦‚æœæ˜¯çˆ¬å–æ‰€æœ‰ç« ç¯€ï¼Œå…ˆè¼‰å…¥ç« ç¯€åˆ—è¡¨ç¢ºå®šç¸½æ•¸
    if end_chapter == float('inf'):
        chapters = scraper.load_chapter_list()
        if chapters:
            end_chapter = len(chapters)
            print(f"ğŸ“š ç¸½å…± {end_chapter} ç« ")
        else:
            print("âŒ ç„¡æ³•è¼‰å…¥ç« ç¯€åˆ—è¡¨")
            return

    # é–‹å§‹çˆ¬å–
    print(f"ğŸš€ é–‹å§‹çˆ¬å–... (å»¶é²: {delay_range[0]}-{delay_range[1]}ç§’)")
    results = scraper.scrape_range(start_chapter, end_chapter, delay_range)

    # é¡¯ç¤ºçµæœ
    if results:
        success_count = sum(1 for r in results if r['status'] == 'success')
        total_count = len(results)
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"   æˆåŠŸ: {success_count}/{total_count}")
        print(f"   è¼¸å‡ºç›®éŒ„: {args.output}")
    else:
        print("\nâŒ çˆ¬å–å¤±æ•—")


if __name__ == "__main__":
    main()