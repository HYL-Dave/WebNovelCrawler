#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†é ç« ç¯€çˆ¬èŸ² - å°ˆé–€è™•ç†Novel543ç­‰æœ‰åˆ†é çš„å°èªªç¶²ç«™
è‡ªå‹•æª¢æ¸¬ä¸¦çˆ¬å–æ‰€æœ‰åˆ†é å…§å®¹
"""

import argparse
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, WebDriverException
import time
import os
import json
import logging
import random
import sys
import re
from pathlib import Path
from urllib.parse import urlparse, urlunparse


class PaginatedNovelScraper:
    def __init__(self, csv_file_path, output_dir="paginated_novels", headless=False):
        """
        åˆå§‹åŒ–åˆ†é å°èªªçˆ¬èŸ²
        """
        self.csv_file_path = csv_file_path
        self.output_dir = output_dir
        self.headless = headless
        self.driver = None

        # åˆ†é æª¢æ¸¬çš„æ­£å‰‡è¡¨é”å¼
        self.pagination_patterns = [
            r'\((\d+)/(\d+)\)',  # (1/2), (2/3) ç­‰
            r'ç¬¬(\d+)é /å…±(\d+)é ',  # ç¬¬1é /å…±3é 
            r'(\d+)/(\d+)é ',  # 1/3é 
        ]

        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        self.setup_logging()

    def setup_logging(self):
        """è¨­ç½®æ—¥èªŒç³»çµ±"""
        log_file = Path(self.output_dir) / 'paginated_scraping.log'

        logging.getLogger().handlers.clear()

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)

    def setup_driver(self):
        """è¨­ç½®Chromeç€è¦½å™¨é©…å‹•"""
        try:
            chrome_options = Options()
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)

            user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            chrome_options.add_argument(f'--user-agent={user_agent}')

            if self.headless:
                chrome_options.add_argument('--headless')
                self.logger.info("é‹è¡Œåœ¨ç„¡é ­æ¨¡å¼")

            try:
                from webdriver_manager.chrome import ChromeDriverManager
                service = Service(ChromeDriverManager().install())
                self.driver = webdriver.Chrome(service=service, options=chrome_options)
            except ImportError:
                self.driver = webdriver.Chrome(options=chrome_options)

            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            self.logger.info("ç€è¦½å™¨é©…å‹•è¨­ç½®æˆåŠŸ")
            return True

        except Exception as e:
            self.logger.error(f"ç€è¦½å™¨é©…å‹•è¨­ç½®å¤±æ•—: {e}")
            return False

    def load_chapter_list(self):
        """è¼‰å…¥ç« ç¯€åˆ—è¡¨"""
        try:
            if not Path(self.csv_file_path).exists():
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°CSVæª”æ¡ˆ: {self.csv_file_path}")

            df = pd.read_csv(self.csv_file_path, encoding='utf-8')
            df = df.dropna()

            chapters = []
            for _, row in df.iterrows():
                title = row['tablescraper-selected-row']
                url = row['tablescraper-selected-row href']
                if title and url:
                    chapters.append({
                        'title': str(title).strip(),
                        'url': str(url).strip()
                    })

            self.logger.info(f"è¼‰å…¥äº† {len(chapters)} å€‹ç« ç¯€")
            return chapters
        except Exception as e:
            self.logger.error(f"è¼‰å…¥CSVæª”æ¡ˆå¤±æ•—: {e}")
            return []

    def detect_pagination(self, title):
        """
        æª¢æ¸¬æ¨™é¡Œä¸­çš„åˆ†é ä¿¡æ¯
        è¿”å›: (current_page, total_pages) æˆ– None
        """
        for pattern in self.pagination_patterns:
            match = re.search(pattern, title)
            if match:
                current_page = int(match.group(1))
                total_pages = int(match.group(2))
                self.logger.debug(f"æª¢æ¸¬åˆ°åˆ†é : {current_page}/{total_pages}")
                return current_page, total_pages
        return None

    def construct_page_url(self, base_url, page_number):
        """
        æ ¹æ“šåŸºç¤URLå’Œé ç¢¼æ§‹é€ åˆ†é URL
        ä¾‹: https://www.novel543.com/0621496793/8096_1.html -> https://www.novel543.com/0621496793/8096_1_2.html
        """
        try:
            # è§£æURL
            parsed = urlparse(base_url)
            path_parts = parsed.path.split('/')

            # æ‰¾åˆ°æœ€å¾Œä¸€å€‹éƒ¨åˆ†ï¼ˆæª”æ¡ˆåï¼‰
            filename = path_parts[-1]

            # è™•ç†ä¸åŒçš„URLæ¨¡å¼
            if '.html' in filename:
                # ç§»é™¤.htmlå¾Œç¶´
                base_name = filename.replace('.html', '')

                # æ§‹é€ æ–°çš„æª”æ¡ˆå
                new_filename = f"{base_name}_{page_number}.html"

                # é‡æ–°æ§‹é€ URL
                path_parts[-1] = new_filename
                new_path = '/'.join(path_parts)

                new_url = urlunparse((
                    parsed.scheme,
                    parsed.netloc,
                    new_path,
                    parsed.params,
                    parsed.query,
                    parsed.fragment
                ))

                self.logger.debug(f"æ§‹é€ åˆ†é URL: {new_url}")
                return new_url
            else:
                self.logger.warning(f"ç„¡æ³•è™•ç†çš„URLæ ¼å¼: {base_url}")
                return None

        except Exception as e:
            self.logger.error(f"æ§‹é€ åˆ†é URLå¤±æ•—: {e}")
            return None

    def extract_content_from_page(self):
        """å¾ç•¶å‰é é¢æå–å…§å®¹"""
        try:
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )

            # é‡å°Novel543çš„ç‰¹å®šé¸æ“‡å™¨
            content_selectors = [
                "#content",
                ".content",
                ".novel-content",
                ".chapter-content",
                ".text-content",
                "div[class*='content']",
                "div[id*='content']",
                ".reading-content",
                "#chapterContent"
            ]

            content = ""
            title = ""

            # å…ˆå˜—è©¦ç²å–æ¨™é¡Œ
            title_selectors = ["h1", ".title", ".chapter-title", "h2", "h3"]
            for selector in title_selectors:
                try:
                    title_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    if title_element:
                        title = title_element.text.strip()
                        break
                except:
                    continue

            # ç²å–å…§å®¹
            for selector in content_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        for element in elements:
                            text = element.text.strip()
                            if len(text) > len(content):
                                content = text
                        if content and len(content) > 100:  # ç¢ºä¿å…§å®¹è¶³å¤ é•·
                            break
                except:
                    continue

            # å¦‚æœé‚„æ˜¯æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨é€šç”¨æ–¹æ³•
            if not content:
                try:
                    divs = self.driver.find_elements(By.TAG_NAME, "div")
                    if divs:
                        content = max(divs, key=lambda div: len(div.text)).text.strip()
                except:
                    pass

            return title, content

        except Exception as e:
            self.logger.error(f"æå–é é¢å…§å®¹å¤±æ•—: {e}")
            return "", ""

    def scrape_paginated_chapter(self, chapter_info):
        """çˆ¬å–åŒ…å«åˆ†é çš„å®Œæ•´ç« ç¯€"""
        try:
            base_url = chapter_info['url']
            chapter_title = chapter_info['title']

            self.logger.info(f"ğŸ” é–‹å§‹åˆ†æç« ç¯€: {chapter_title}")

            # è¨ªå•ç¬¬ä¸€é 
            self.driver.get(base_url)
            time.sleep(random.uniform(2, 4))

            # ç²å–ç¬¬ä¸€é çš„æ¨™é¡Œå’Œå…§å®¹
            page_title, page_content = self.extract_content_from_page()

            if not page_content:
                self.logger.warning(f"âŒ ç„¡æ³•ç²å–ç¬¬ä¸€é å…§å®¹: {chapter_title}")
                return {
                    'title': chapter_title,
                    'url': base_url,
                    'content': '',
                    'status': 'no_content',
                    'pages': 0
                }

            # æª¢æ¸¬æ˜¯å¦æœ‰åˆ†é 
            pagination_info = self.detect_pagination(page_title)

            if pagination_info is None:
                # æ²’æœ‰åˆ†é ï¼Œç›´æ¥è¿”å›
                self.logger.info(f"âœ… å–®é ç« ç¯€: {chapter_title}")
                return {
                    'title': chapter_title,
                    'url': base_url,
                    'content': page_content,
                    'status': 'success',
                    'pages': 1
                }

            # æœ‰åˆ†é ï¼Œç²å–æ‰€æœ‰é é¢
            current_page, total_pages = pagination_info
            self.logger.info(f"ğŸ“„ æª¢æ¸¬åˆ°åˆ†é ç« ç¯€: {total_pages} é ")

            all_content = [page_content]  # ç¬¬ä¸€é å…§å®¹
            failed_pages = []

            # çˆ¬å–å‰©é¤˜é é¢
            for page_num in range(2, total_pages + 1):
                try:
                    page_url = self.construct_page_url(base_url, page_num)
                    if not page_url:
                        failed_pages.append(page_num)
                        continue

                    self.logger.info(f"  ğŸ“– çˆ¬å–ç¬¬ {page_num}/{total_pages} é ...")
                    self.driver.get(page_url)
                    time.sleep(random.uniform(1, 3))

                    _, content = self.extract_content_from_page()

                    if content:
                        all_content.append(content)
                        self.logger.debug(f"    âœ… ç¬¬{page_num}é æˆåŠŸ (é•·åº¦: {len(content)})")
                    else:
                        failed_pages.append(page_num)
                        self.logger.warning(f"    âŒ ç¬¬{page_num}é å…§å®¹ç‚ºç©º")

                except Exception as e:
                    failed_pages.append(page_num)
                    self.logger.error(f"    âŒ ç¬¬{page_num}é çˆ¬å–å¤±æ•—: {e}")

            # åˆä½µæ‰€æœ‰å…§å®¹
            combined_content = '\n\n'.join(all_content)

            success_pages = total_pages - len(failed_pages)
            status = 'success' if success_pages >= total_pages * 0.8 else 'partial_success'

            self.logger.info(f"ğŸ‰ ç« ç¯€å®Œæˆ: {success_pages}/{total_pages} é æˆåŠŸ")

            return {
                'title': chapter_title,
                'url': base_url,
                'content': combined_content,
                'status': status,
                'pages': success_pages,
                'total_pages': total_pages,
                'failed_pages': failed_pages
            }

        except Exception as e:
            self.logger.error(f"âŒ ç« ç¯€çˆ¬å–å¤±æ•— {chapter_info['title']}: {e}")
            return {
                'title': chapter_info['title'],
                'url': chapter_info['url'],
                'content': '',
                'status': 'error',
                'pages': 0,
                'error': str(e)
            }

    def save_chapter(self, chapter_data, chapter_num):
        """ä¿å­˜ç« ç¯€å…§å®¹"""
        try:
            safe_title = "".join(
                c for c in chapter_data['title'] if c.isalnum() or c in (' ', '-', '_', 'ï¼', 'ï¼Ÿ')).rstrip()
            filename = f"{chapter_num:03d}_{safe_title[:50]}.txt"
            filepath = Path(self.output_dir) / filename

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(f"æ¨™é¡Œ: {chapter_data['title']}\n")
                f.write(f"ç¶²å€: {chapter_data['url']}\n")
                f.write(f"ç‹€æ…‹: {chapter_data['status']}\n")
                f.write(f"ç« ç¯€ç·¨è™Ÿ: {chapter_num}\n")
                f.write(f"ç¸½é æ•¸: {chapter_data.get('total_pages', 1)}\n")
                f.write(f"æˆåŠŸé æ•¸: {chapter_data.get('pages', 1)}\n")
                if chapter_data.get('failed_pages'):
                    f.write(f"å¤±æ•—é æ•¸: {chapter_data['failed_pages']}\n")
                f.write("-" * 50 + "\n\n")
                f.write(chapter_data['content'])

            return str(filepath)
        except Exception as e:
            self.logger.error(f"ä¿å­˜ç« ç¯€å¤±æ•—: {e}")
            return None

    def scrape_range(self, start_chapter=1, end_chapter=5, delay_range=(3, 6)):
        """çˆ¬å–æŒ‡å®šç¯„åœçš„ç« ç¯€"""
        if not self.setup_driver():
            return []

        try:
            chapters = self.load_chapter_list()
            if not chapters:
                return []

            total_chapters = len(chapters)
            end_chapter = min(end_chapter, total_chapters)

            selected_chapters = chapters[start_chapter - 1:end_chapter]
            self.logger.info(f"ğŸš€ é–‹å§‹çˆ¬å–ç¬¬ {start_chapter} åˆ°ç¬¬ {end_chapter} ç« ï¼Œå…± {len(selected_chapters)} ç« ")

            results = []
            success_count = 0
            total_pages = 0

            for i, chapter_info in enumerate(selected_chapters, start_chapter):
                result = self.scrape_paginated_chapter(chapter_info)

                if result['status'] in ['success', 'partial_success']:
                    filepath = self.save_chapter(result, i)
                    if filepath:
                        result['saved_path'] = filepath
                        success_count += 1
                        total_pages += result.get('pages', 0)

                results.append(result)

                # é€²åº¦å ±å‘Š
                progress = f"[{i - start_chapter + 1}/{len(selected_chapters)}]"
                pages_info = f"(å…±çˆ¬å– {total_pages} é )"
                self.logger.info(f"{progress} é€²åº¦æ›´æ–° - æˆåŠŸ: {success_count} {pages_info}")

                # å»¶é²
                if i < end_chapter:
                    delay = random.uniform(delay_range[0], delay_range[1])
                    time.sleep(delay)

            # ä¿å­˜æ‘˜è¦
            self.save_summary(results, start_chapter, end_chapter, total_pages)

            failed_count = len(results) - success_count
            self.logger.info(f"ğŸ‰ çˆ¬å–å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±æ•—: {failed_count}, ç¸½é æ•¸: {total_pages}")

            return results

        finally:
            if self.driver:
                self.driver.quit()

    def save_summary(self, results, start_chapter, end_chapter, total_pages):
        """ä¿å­˜çˆ¬å–çµæœæ‘˜è¦"""
        summary = {
            'range': f"{start_chapter}-{end_chapter}",
            'total_chapters': len(results),
            'successful_chapters': sum(1 for r in results if r['status'] in ['success', 'partial_success']),
            'total_pages_scraped': total_pages,
            'average_pages_per_chapter': total_pages / len(results) if results else 0,
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'chapter_details': [
                {
                    'title': r['title'],
                    'status': r['status'],
                    'pages': r.get('pages', 0),
                    'total_pages': r.get('total_pages', 1)
                } for r in results
            ]
        }

        summary_path = Path(self.output_dir) / f'paginated_summary_{start_chapter}-{end_chapter}.json'
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)


def main():
    parser = argparse.ArgumentParser(description='åˆ†é å°èªªçˆ¬èŸ²')
    parser.add_argument('csv_file', help='CSVæª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--start', '-s', type=int, default=1, help='é–‹å§‹ç« ç¯€')
    parser.add_argument('--end', '-e', type=int, default=5, help='çµæŸç« ç¯€')
    parser.add_argument('--output', '-o', default='paginated_novels', help='è¼¸å‡ºç›®éŒ„')
    parser.add_argument('--delay', '-d', default='3-6', help='å»¶é²æ™‚é–“ç¯„åœ')
    parser.add_argument('--headless', action='store_true', help='ç„¡é ­æ¨¡å¼')
    parser.add_argument('--test', action='store_true', help='æ¸¬è©¦æ¨¡å¼ï¼ˆå‰3ç« ï¼‰')

    args = parser.parse_args()

    # è§£æå»¶é²ç¯„åœ
    try:
        if '-' in args.delay:
            min_delay, max_delay = map(float, args.delay.split('-'))
            delay_range = (min_delay, max_delay)
        else:
            delay = float(args.delay)
            delay_range = (delay, delay + 2)
    except:
        delay_range = (3, 6)

    # æ¸¬è©¦æ¨¡å¼
    if args.test:
        start_chapter, end_chapter = 1, 3
        print("ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šçˆ¬å–å‰3ç« ")
    else:
        start_chapter, end_chapter = args.start, args.end

    print(f"ğŸ” åˆ†é å°èªªçˆ¬èŸ²å•Ÿå‹•")
    print(f"ğŸ“– çˆ¬å–ç¯„åœ: ç¬¬{start_chapter}-{end_chapter}ç« ")
    print(f"â±ï¸  å»¶é²è¨­ç½®: {delay_range[0]}-{delay_range[1]}ç§’")

    scraper = PaginatedNovelScraper(
        csv_file_path=args.csv_file,
        output_dir=args.output,
        headless=args.headless
    )

    results = scraper.scrape_range(start_chapter, end_chapter, delay_range)

    if results:
        success_count = sum(1 for r in results if r['status'] in ['success', 'partial_success'])
        total_pages = sum(r.get('pages', 0) for r in results)
        print(f"\nğŸ‰ çˆ¬å–å®Œæˆï¼")
        print(f"   æˆåŠŸç« ç¯€: {success_count}/{len(results)}")
        print(f"   ç¸½é æ•¸: {total_pages}")
        print(f"   è¼¸å‡ºç›®éŒ„: {args.output}")


if __name__ == "__main__":
    main()